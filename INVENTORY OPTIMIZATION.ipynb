{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55be592b-d0e8-4432-be81-eb364b9e93ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded: 600 rows, 14 columns\n",
      "\n",
      "Data overview:\n",
      "SKU_ID                    object\n",
      "Item_Cost                float64\n",
      "Item_Count               float64\n",
      "Total_Cost               float64\n",
      "Lead_Time                float64\n",
      "Shelf_Life               float64\n",
      "EOQ                      float64\n",
      "Lead_Time_Variability     object\n",
      "Seasonality               object\n",
      "Warehouse_Location        object\n",
      "Customer_Reviews         float64\n",
      "Historical_Sales_Data     object\n",
      "Demand_Fluctuation        object\n",
      "ABC_Classification        object\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "SKU_ID                    0\n",
      "Item_Cost                 1\n",
      "Item_Count               16\n",
      "Total_Cost               10\n",
      "Lead_Time                14\n",
      "Shelf_Life                6\n",
      "EOQ                       4\n",
      "Lead_Time_Variability     5\n",
      "Seasonality               5\n",
      "Warehouse_Location        1\n",
      "Customer_Reviews         12\n",
      "Historical_Sales_Data    15\n",
      "Demand_Fluctuation        7\n",
      "ABC_Classification       24\n",
      "dtype: int64\n",
      "\n",
      "Preprocessing NaN values...\n",
      "Filled NaN in Item_Cost with median: 11.6\n",
      "Filled NaN in Item_Count with median: 150.0\n",
      "Filled NaN in Total_Cost with median: 1800.0\n",
      "Filled NaN in Lead_Time with median: 5.0\n",
      "Filled NaN in Shelf_Life with median: 180.0\n",
      "Filled NaN in EOQ with median: 150.0\n",
      "Filled NaN in Customer_Reviews with median: 4.1\n",
      "Filled NaN in Lead_Time_Variability with mode: medium\n",
      "Filled NaN in Seasonality with mode: none\n",
      "Filled NaN in Warehouse_Location with mode: A1\n",
      "Filled NaN in Historical_Sales_Data with mode: 180\n",
      "Filled NaN in Demand_Fluctuation with mode: increasing\n",
      "Filled NaN in ABC_Classification with mode: C\n",
      "Remaining NaN values: 0\n",
      "\n",
      "Starting model optimization...\n",
      "\n",
      "Handling missing values before processing...\n",
      "Missing values per column before preprocessing:\n",
      "Series([], dtype: int64)\n",
      "Training set: (480, 13), Test set: (120, 13)\n",
      "Class distribution:\n",
      "ABC_Classification\n",
      "C    0.391667\n",
      "B    0.306250\n",
      "A    0.302083\n",
      "Name: proportion, dtype: float64\n",
      "Preprocessing training data...\n",
      "Processed training data shape: (480, 552)\n",
      "Resampled training data shape: (564, 552)\n",
      "Optimizing SVM...\n",
      "SVM Best Params: OrderedDict([('C', 1.6994636371262763), ('gamma', 0.15246748254295628), ('kernel', 'poly')])\n",
      "SVM Best Score: 0.9007\n",
      "Optimizing KNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\OneDrive\\Documents\\anaconda\\Lib\\site-packages\\skopt\\space\\space.py:116: UserWarning: Dimension [1, 2] was inferred to Integer(low=1, high=2, prior='uniform', transform='identity'). In upcoming versions of scikit-optimize, it will be inferred to Categorical(categories=(1, 2), prior=None). See the documentation of the check_dimension function for the upcoming API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dell\\OneDrive\\Documents\\anaconda\\Lib\\site-packages\\skopt\\space\\space.py:116: UserWarning: Dimension [1, 2] was inferred to Integer(low=1, high=2, prior='uniform', transform='identity'). In upcoming versions of scikit-optimize, it will be inferred to Categorical(categories=(1, 2), prior=None). See the documentation of the check_dimension function for the upcoming API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dell\\OneDrive\\Documents\\anaconda\\Lib\\site-packages\\skopt\\space\\space.py:116: UserWarning: Dimension [1, 2] was inferred to Integer(low=1, high=2, prior='uniform', transform='identity'). In upcoming versions of scikit-optimize, it will be inferred to Categorical(categories=(1, 2), prior=None). See the documentation of the check_dimension function for the upcoming API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Best Params: OrderedDict([('n_neighbors', 1), ('p', 2), ('weights', 'distance')])\n",
      "KNN Best Score: 0.8599\n",
      "Optimizing NaiveBayes...\n",
      "NaiveBayes Best Params: OrderedDict([('var_smoothing', 0.04403762515730915)])\n",
      "NaiveBayes Best Score: 0.7837\n",
      "Optimizing RandomForest...\n",
      "RandomForest Best Params: OrderedDict([('max_depth', 24), ('n_estimators', 463)])\n",
      "RandomForest Best Score: 0.9096\n",
      "Optimizing DecisionTree...\n",
      "DecisionTree Best Params: OrderedDict([('max_depth', 41), ('min_samples_leaf', 4), ('min_samples_split', 13)])\n",
      "DecisionTree Best Score: 0.8262\n",
      "Training final ensemble model...\n",
      "Final Ensemble Model Performance:\n",
      "Accuracy: 0.9000\n",
      "Cohen's Kappa: 0.8485\n",
      "MCC: 0.8488\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       0.91      0.86      0.89        36\n",
      "           B       0.92      0.92      0.92        37\n",
      "           C       0.88      0.91      0.90        47\n",
      "\n",
      "    accuracy                           0.90       120\n",
      "   macro avg       0.90      0.90      0.90       120\n",
      "weighted avg       0.90      0.90      0.90       120\n",
      "\n",
      "\n",
      "Individual Model Performances:\n",
      "\n",
      "SVM Performance:\n",
      "Accuracy: 0.9500\n",
      "Cohen's Kappa: 0.9245\n",
      "MCC: 0.9248\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       0.92      0.97      0.95        36\n",
      "           B       0.97      0.95      0.96        37\n",
      "           C       0.96      0.94      0.95        47\n",
      "\n",
      "    accuracy                           0.95       120\n",
      "   macro avg       0.95      0.95      0.95       120\n",
      "weighted avg       0.95      0.95      0.95       120\n",
      "\n",
      "\n",
      "KNN Performance:\n",
      "Accuracy: 0.8750\n",
      "Cohen's Kappa: 0.8120\n",
      "MCC: 0.8153\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       0.88      0.83      0.86        36\n",
      "           B       0.82      0.97      0.89        37\n",
      "           C       0.93      0.83      0.88        47\n",
      "\n",
      "    accuracy                           0.88       120\n",
      "   macro avg       0.88      0.88      0.87       120\n",
      "weighted avg       0.88      0.88      0.87       120\n",
      "\n",
      "\n",
      "NaiveBayes Performance:\n",
      "Accuracy: 0.7250\n",
      "Cohen's Kappa: 0.5890\n",
      "MCC: 0.5972\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       0.80      0.78      0.79        36\n",
      "           B       0.63      0.84      0.72        37\n",
      "           C       0.78      0.60      0.67        47\n",
      "\n",
      "    accuracy                           0.72       120\n",
      "   macro avg       0.74      0.74      0.73       120\n",
      "weighted avg       0.74      0.72      0.72       120\n",
      "\n",
      "\n",
      "RandomForest Performance:\n",
      "Accuracy: 0.9167\n",
      "Cohen's Kappa: 0.8739\n",
      "MCC: 0.8742\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       0.89      0.92      0.90        36\n",
      "           B       0.97      0.92      0.94        37\n",
      "           C       0.90      0.91      0.91        47\n",
      "\n",
      "    accuracy                           0.92       120\n",
      "   macro avg       0.92      0.92      0.92       120\n",
      "weighted avg       0.92      0.92      0.92       120\n",
      "\n",
      "\n",
      "DecisionTree Performance:\n",
      "Accuracy: 0.8833\n",
      "Cohen's Kappa: 0.8242\n",
      "MCC: 0.8250\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       0.87      0.94      0.91        36\n",
      "           B       0.86      0.86      0.86        37\n",
      "           C       0.91      0.85      0.88        47\n",
      "\n",
      "    accuracy                           0.88       120\n",
      "   macro avg       0.88      0.89      0.88       120\n",
      "weighted avg       0.88      0.88      0.88       120\n",
      "\n",
      "\n",
      "Best model achieved 0.9000 accuracy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, cohen_kappa_score, matthews_corrcoef\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# 1. Enhanced Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    # Create copies to avoid modifying the original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # First ensure target column is not null\n",
    "    if df['ABC_Classification'].isnull().any():\n",
    "        print(\"Dropping rows with missing target values\")\n",
    "        df = df.dropna(subset=['ABC_Classification'])\n",
    "    \n",
    "    X = df.drop('ABC_Classification', axis=1)\n",
    "    y = df['ABC_Classification']\n",
    "    \n",
    "    # Identify column types\n",
    "    numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "    \n",
    "    # Create transformers for numeric and categorical features\n",
    "    transformers = []\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=True))\n",
    "        ])\n",
    "        transformers.append(('num', numeric_transformer, numeric_cols))\n",
    "    \n",
    "    if len(categorical_cols) > 0:\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "        transformers.append(('cat', categorical_transformer, categorical_cols))\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers)\n",
    "    \n",
    "    return X, y, preprocessor\n",
    "\n",
    "# 2. Train Model with Bayesian Optimization\n",
    "def optimize_models(X_train, y_train, preprocessor):\n",
    "    # Process the training data once to avoid repeated preprocessing\n",
    "    print(\"Preprocessing training data...\")\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    print(f\"Processed training data shape: {X_train_processed.shape}\")\n",
    "    \n",
    "    # Create SMOTE instance outside the pipeline\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "    print(f\"Resampled training data shape: {X_train_resampled.shape}\")\n",
    "    \n",
    "    classifiers = {\n",
    "        'SVM': SVC(probability=True, random_state=42),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'NaiveBayes': GaussianNB(),\n",
    "        'RandomForest': RandomForestClassifier(random_state=42),\n",
    "        'DecisionTree': DecisionTreeClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    param_spaces = {\n",
    "        'SVM': {\n",
    "            'C': (0.1, 100, 'log-uniform'), \n",
    "            'gamma': (0.001, 1, 'log-uniform'), \n",
    "            'kernel': ['rbf', 'poly']\n",
    "        },\n",
    "        'KNN': {\n",
    "            'n_neighbors': (1, 30),\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'p': [1, 2]  # 1 for Manhattan, 2 for Euclidean\n",
    "        },\n",
    "        'NaiveBayes': {\n",
    "            'var_smoothing': (1e-9, 1e-1, 'log-uniform')\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'n_estimators': (50, 500), \n",
    "            'max_depth': (3, 50)\n",
    "        },\n",
    "        'DecisionTree': {\n",
    "            'max_depth': (3, 50),\n",
    "            'min_samples_split': (2, 20),\n",
    "            'min_samples_leaf': (1, 20)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    best_models = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"Optimizing {name}...\")\n",
    "        \n",
    "        # Use BayesSearchCV directly on the classifier with preprocessed data\n",
    "        optimizer = BayesSearchCV(\n",
    "            clf, \n",
    "            param_spaces[name], \n",
    "            n_iter=10,\n",
    "            cv=3,\n",
    "            scoring='accuracy', \n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        optimizer.fit(X_train_resampled, y_train_resampled)\n",
    "        best_models[name] = optimizer.best_estimator_\n",
    "        \n",
    "        print(f\"{name} Best Params: {optimizer.best_params_}\")\n",
    "        print(f\"{name} Best Score: {optimizer.best_score_:.4f}\")\n",
    "    \n",
    "    return best_models, preprocessor, smote\n",
    "\n",
    "# 3. Create Ensemble\n",
    "def create_ensemble(best_models):\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('svm', best_models['SVM']),\n",
    "            ('knn', best_models['KNN']),\n",
    "            ('nb', best_models['NaiveBayes']),\n",
    "            ('rf', best_models['RandomForest']),\n",
    "            ('dt', best_models['DecisionTree'])\n",
    "        ], \n",
    "        voting='soft'\n",
    "    )\n",
    "    return ensemble\n",
    "\n",
    "# 4. Evaluate Model\n",
    "def evaluate_model(model, X_test_transformed, y_test):\n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    return accuracy\n",
    "\n",
    "# Main Execution Function\n",
    "def improve_model_performance(df):\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Check if target exists\n",
    "    if 'ABC_Classification' not in df.columns:\n",
    "        raise ValueError(\"Target column 'ABC_Classification' not found in dataset\")\n",
    "    \n",
    "    # Handle NaN values explicitly before modeling\n",
    "    print(\"\\nHandling missing values before processing...\")\n",
    "    \n",
    "    # Report on missing values\n",
    "    missing_counts = df.isnull().sum()\n",
    "    print(\"Missing values per column before preprocessing:\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    \n",
    "    # Drop rows where target is NaN\n",
    "    if df['ABC_Classification'].isnull().any():\n",
    "        df = df.dropna(subset=['ABC_Classification'])\n",
    "        print(f\"Dropped rows with missing target values. Remaining rows: {df.shape[0]}\")\n",
    "    \n",
    "    # Preprocess data\n",
    "    X, y, preprocessor = preprocess_data(df)\n",
    "    \n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Print dataset shapes\n",
    "    print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Get baseline model performance\n",
    "    print(\"Class distribution:\")\n",
    "    print(y_train.value_counts(normalize=True))\n",
    "    \n",
    "    # Optimize models - now returns preprocessor and smote\n",
    "    best_models, fitted_preprocessor, fitted_smote = optimize_models(X_train, y_train, preprocessor)\n",
    "    \n",
    "    # Create ensemble\n",
    "    ensemble_model = create_ensemble(best_models)\n",
    "    \n",
    "    # Transform test data using the fitted preprocessor\n",
    "    X_test_transformed = fitted_preprocessor.transform(X_test)\n",
    "    \n",
    "    # Train the ensemble on preprocessed and resampled data\n",
    "    print(\"Training final ensemble model...\")\n",
    "    X_train_processed = fitted_preprocessor.transform(X_train)\n",
    "    X_train_resampled, y_train_resampled = fitted_smote.fit_resample(X_train_processed, y_train)\n",
    "    ensemble_model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Evaluate the final model\n",
    "    print(\"Final Ensemble Model Performance:\")\n",
    "    final_accuracy = evaluate_model(ensemble_model, X_test_transformed, y_test)\n",
    "    \n",
    "    # Also evaluate individual models\n",
    "    print(\"\\nIndividual Model Performances:\")\n",
    "    for name, model in best_models.items():\n",
    "        print(f\"\\n{name} Performance:\")\n",
    "        _ = evaluate_model(model, X_test_transformed, y_test)\n",
    "    \n",
    "    # Return the full pipeline components\n",
    "    return ensemble_model, fitted_preprocessor, fitted_smote, final_accuracy\n",
    "\n",
    "# Load Dataset and run\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load data with error handling\n",
    "        print(\"Loading dataset...\")\n",
    "        df = pd.read_csv(\"Inventory_dataset (2).csv\")\n",
    "        df=df.head(600)\n",
    "        print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        \n",
    "        # Only use first 600 rows if specified\n",
    "        df = df.head(600)\n",
    "        \n",
    "        # Quick data overview\n",
    "        print(\"\\nData overview:\")\n",
    "        print(df.dtypes)\n",
    "        print(\"\\nMissing values per column:\")\n",
    "        print(df.isnull().sum())\n",
    "        \n",
    "        # Preprocess NaN values for the entire dataset\n",
    "        print(\"\\nPreprocessing NaN values...\")\n",
    "        \n",
    "        # 1. For numeric columns, replace NaN with median\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "        for col in numeric_cols:\n",
    "            if df[col].isnull().any():\n",
    "                median_val = df[col].median()\n",
    "                df[col] = df[col].fillna(median_val)\n",
    "                print(f\"Filled NaN in {col} with median: {median_val}\")\n",
    "        \n",
    "        # 2. For categorical columns, replace NaN with mode\n",
    "        categorical_cols = df.select_dtypes(exclude=['number']).columns\n",
    "        for col in categorical_cols:\n",
    "            if df[col].isnull().any():\n",
    "                mode_val = df[col].mode()[0]\n",
    "                df[col] = df[col].fillna(mode_val)\n",
    "                print(f\"Filled NaN in {col} with mode: {mode_val}\")\n",
    "        \n",
    "        # Verify NaN values are handled\n",
    "        remaining_nans = df.isnull().sum().sum()\n",
    "        print(f\"Remaining NaN values: {remaining_nans}\")\n",
    "        \n",
    "        if remaining_nans > 0:\n",
    "            print(\"Warning: Still have NaN values. Removing rows with NaN...\")\n",
    "            df = df.dropna()\n",
    "            print(f\"After dropping NaNs: {df.shape[0]} rows remaining\")\n",
    "        \n",
    "        # Run the optimization\n",
    "        print(\"\\nStarting model optimization...\")\n",
    "        ensemble_model, preprocessor, smote, best_accuracy = improve_model_performance(df)\n",
    "        print(f\"\\nBest model achieved {best_accuracy:.4f} accuracy\")\n",
    "        \n",
    "        # Save the model components if needed\n",
    "        # import joblib\n",
    "        # joblib.dump(ensemble_model, 'ensemble_model.pkl')\n",
    "        # joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "        # joblib.dump(smote, 'smote.pkl')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5c20d-3e58-4ffa-bd3e-57a5506f55d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
